{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiramdeeP1603/text-generation/blob/main/TextGenratoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jS2wTF6pBwnh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAQ text data"
      ],
      "metadata": {
        "id": "wPxrDBQQCaOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JZ6qUaRt7yQD"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"About the New Program\n",
        "What is the cost for the Advanced Machine Learning Program (AMLP 2024)?\n",
        "The program fee is structured on a quarterly basis, with payments of $999 per quarter.\n",
        "How long does the program last?\n",
        "The total duration of the program spans 9 months. Hence, the total fee amounts to $999 * 3 = $2997.\n",
        "What topics are covered in this mentorship program?\n",
        "The curriculum includes:\n",
        "- Advanced Python for Machine Learning\n",
        "- Deep Learning Frameworks\n",
        "- Natural Language Processing (NLP)\n",
        "- Computer Vision\n",
        "- Reinforcement Learning\n",
        "- Time Series Analysis\n",
        "- Deployment Strategies\n",
        "- Project Work\n",
        "For a detailed syllabus, please visit our course page: https://learnwith.campusx.in/courses/CampusX-Advanced-Machine-Learning-Program-2024\n",
        "Are there any recordings available if I miss a live session?\n",
        "Yes, all sessions are recorded, enabling you to review them at your convenience.\n",
        "Where can I access the class timetable?\n",
        "You can view the monthly timetable on our Google Sheets schedule: https://docs.google.com/spreadsheets/d/1aBcDeF2GhIjKlMnOpQrStUvWxYz/edit?usp=sharing.\n",
        "What is the duration of each live session?\n",
        "Typically, each session lasts around 90 minutes.\n",
        "What language is used during the sessions?\n",
        "The sessions are conducted primarily in English, with explanations provided in simple terms for easier understanding.\n",
        "How will I be notified about upcoming classes?\n",
        "You will receive email notifications before each scheduled session once you've enrolled.\n",
        "Is this course suitable for non-technical backgrounds?\n",
        "Yes, the program is designed to accommodate learners from diverse educational backgrounds.\n",
        "Can I join the program midway?\n",
        "Absolutely, you have the flexibility to join at any point in the program's duration.\n",
        "If I enroll later, will I have access to previous lectures?\n",
        "Yes, upon enrollment, you gain access to all past lectures and materials via your dashboard.\n",
        "Where do I submit assignments?\n",
        "There are no formal assignments. Instead, we provide hands-on projects for self-evaluation and learning.\n",
        "Will there be practical case studies in the program?\n",
        "Yes, practical case studies are an integral part of the curriculum.\n",
        "How can I contact support?\n",
        "You can reach out to us via email at support@campusx.in for any queries or assistance.\n",
        "Payment and Registration Details\n",
        "Where should I make my payments? Is it through your YouTube channel or website?\n",
        "All payments should be made securely on our website: https://learnwith.campusx.in/.\n",
        "Can I pay the entire $2997 fee upfront?\n",
        "Yes, you have the option to pay the full amount upon registration.\n",
        "What is the validity period for quarterly subscriptions? For instance, if I pay on April 15th, do I need to pay again on May 1st or May 15th?\n",
        "The validity period extends for 90 days from the date of payment. Hence, if you enroll on April 15th, your next payment is due on July 15th.\n",
        "What is the refund policy if I'm dissatisfied with the course?\n",
        "You are entitled to a full refund within 10 days of payment, no questions asked.\n",
        "I reside outside the United States and cannot make payments on your website. What are my options?\n",
        "Please contact us via email at international@campusx.in for alternative payment arrangements.\n",
        "Post-Registration Queries\n",
        "Until when can I access paid videos on your platform?\n",
        "You can access all paid content until your subscription remains active. For example, if you subscribe on June 1st, you can view all paid sessions from June 1st to June 30th. Subsequently, a renewal is required.\n",
        "Once I complete the course and pay the full fee of $2997 (or 3 installments of $999 each), how long can I continue to access the paid sessions?\n",
        "You will retain access to paid sessions until December 2024.\n",
        "Why is there no lifetime access provided?\n",
        "This decision is based on our commitment to keeping course fees affordable for all learners.\n",
        "Who can I contact for doubts after sessions?\n",
        "Please fill out the provided Google Form in your dashboard for personalized one-on-one doubt-clearing sessions with our team.\n",
        "If I join the program late, can I still seek clarification on doubts from previous weeks?\n",
        "Certainly, select the option for past-week doubts in the Google Form for doubt clearance.\n",
        "Certificate and Placement Assistance\n",
        "What are the requirements to receive a certificate?\n",
        "To qualify for a certificate, you must complete all course assessments and fulfill the total fee payment of $2997.\n",
        "I'm joining the program later. How can I settle payments for previous months?\n",
        "Upon payment for the current quarter, you will receive a link in your dashboard to settle fees for prior months.\n",
        "Does your program offer placement assistance?\n",
        "While we do not guarantee job placements or interviews, our placement assistance includes:\n",
        "- Building a Professional Portfolio\n",
        "- Soft Skills Training\n",
        "- Mentorship Sessions with Industry Experts\n",
        "- Job Search Strategy Discussions\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Tokenize the text"
      ],
      "metadata": {
        "id": "Mw6YsimYCgft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([faqs])"
      ],
      "metadata": {
        "id": "r3Z0eTPQB5ee"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Convert sentences to sequences"
      ],
      "metadata": {
        "id": "L3aperctCl5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = []\n",
        "sentences = faqs.split('\\n')\n",
        "for sentence in sentences:\n",
        "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    for i in range(1, len(tokenized_sentence)):\n",
        "        input_sentences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "6MFnlLn8B8w8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Pad sequences"
      ],
      "metadata": {
        "id": "pX8NjetzCocv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(seq) for seq in input_sentences])\n",
        "pad_sentences = pad_sequences(input_sentences, padding='pre', maxlen=max_len)"
      ],
      "metadata": {
        "id": "CKJ2QcuECBr3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Prepare input (X) and output (y) data"
      ],
      "metadata": {
        "id": "aPy9qwLXCveX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pad_sentences[:, :-1]\n",
        "y = pad_sentences[:, -1]\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)"
      ],
      "metadata": {
        "id": "VVWVCYotCGmK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Define the model"
      ],
      "metadata": {
        "id": "RZppNNtwC0C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len - 1))\n",
        "model.add(LSTM(150, return_sequences=False))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))"
      ],
      "metadata": {
        "id": "wYcetL2XCKII"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Compile the model"
      ],
      "metadata": {
        "id": "DApKDKA9C3gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "znb27BOcCNsK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Train the model"
      ],
      "metadata": {
        "id": "bg_Qt4qHC6cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6he_lgECQKO",
        "outputId": "c5b302da-cf41-46c5-8823-2237e7e3acb9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - 3s 74ms/step - loss: 5.7847 - accuracy: 0.0368\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - 1s 72ms/step - loss: 5.5113 - accuracy: 0.0495\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - 2s 144ms/step - loss: 5.3617 - accuracy: 0.0368\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - 2s 148ms/step - loss: 5.3088 - accuracy: 0.0424\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - 2s 127ms/step - loss: 5.2911 - accuracy: 0.0495\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - 2s 145ms/step - loss: 5.2798 - accuracy: 0.0495\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - 2s 157ms/step - loss: 5.2678 - accuracy: 0.0509\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - 2s 138ms/step - loss: 5.2601 - accuracy: 0.0495\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - 2s 153ms/step - loss: 5.2425 - accuracy: 0.0495\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - 2s 167ms/step - loss: 5.1986 - accuracy: 0.0509\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - 2s 149ms/step - loss: 5.1572 - accuracy: 0.0608\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - 2s 148ms/step - loss: 5.1085 - accuracy: 0.0552\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - 2s 165ms/step - loss: 5.0574 - accuracy: 0.0608\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - 2s 147ms/step - loss: 4.9824 - accuracy: 0.0608\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - 2s 149ms/step - loss: 4.9192 - accuracy: 0.0651\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - 2s 144ms/step - loss: 4.8431 - accuracy: 0.0707\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - 1s 80ms/step - loss: 4.7676 - accuracy: 0.1089\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 4.6825 - accuracy: 0.0905\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - 1s 93ms/step - loss: 4.5787 - accuracy: 0.1174\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 4.4831 - accuracy: 0.1075\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - 2s 162ms/step - loss: 4.3767 - accuracy: 0.1344\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - 2s 138ms/step - loss: 4.2898 - accuracy: 0.1499\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - 1s 94ms/step - loss: 4.1802 - accuracy: 0.1570\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 4.0890 - accuracy: 0.1330\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - 1s 81ms/step - loss: 3.9921 - accuracy: 0.1471\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 3.8679 - accuracy: 0.1895\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 3.7705 - accuracy: 0.1924\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - 1s 75ms/step - loss: 3.6818 - accuracy: 0.1994\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - 1s 75ms/step - loss: 3.5675 - accuracy: 0.2192\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - 1s 72ms/step - loss: 3.4640 - accuracy: 0.2405\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 3.3848 - accuracy: 0.2603\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 3.2949 - accuracy: 0.2956\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - 2s 137ms/step - loss: 3.1945 - accuracy: 0.3126\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - 1s 108ms/step - loss: 3.1209 - accuracy: 0.3013\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 3.0238 - accuracy: 0.3182\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - 1s 73ms/step - loss: 2.9231 - accuracy: 0.3536\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - 1s 75ms/step - loss: 2.8386 - accuracy: 0.3692\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 2.7567 - accuracy: 0.4017\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - 1s 73ms/step - loss: 2.6709 - accuracy: 0.4130\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 2.5778 - accuracy: 0.4611\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - 1s 75ms/step - loss: 2.5136 - accuracy: 0.4724\n",
            "Epoch 42/50\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 2.4373 - accuracy: 0.4965\n",
            "Epoch 43/50\n",
            "12/12 [==============================] - 1s 75ms/step - loss: 2.3480 - accuracy: 0.5106\n",
            "Epoch 44/50\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 2.2674 - accuracy: 0.5290\n",
            "Epoch 45/50\n",
            "12/12 [==============================] - 1s 115ms/step - loss: 2.1962 - accuracy: 0.5757\n",
            "Epoch 46/50\n",
            "12/12 [==============================] - 2s 144ms/step - loss: 2.1311 - accuracy: 0.5870\n",
            "Epoch 47/50\n",
            "12/12 [==============================] - 1s 85ms/step - loss: 2.0615 - accuracy: 0.5955\n",
            "Epoch 48/50\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 2.0009 - accuracy: 0.6011\n",
            "Epoch 49/50\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 1.9426 - accuracy: 0.6195\n",
            "Epoch 50/50\n",
            "12/12 [==============================] - 1s 77ms/step - loss: 1.8743 - accuracy: 0.6464\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f0e333c35e0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Text generation"
      ],
      "metadata": {
        "id": "G0UWoI2YC_9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words=10):\n",
        "    for _ in range(next_words):\n",
        "        token_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        padded_token_text = pad_sequences([token_text], maxlen=max_len - 1, padding='pre')\n",
        "        predicted_word_index = np.argmax(model.predict(padded_token_text), axis=-1)[0]\n",
        "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "oYYm7uXiCTFB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example usage of text generation"
      ],
      "metadata": {
        "id": "242ClG2ADCvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"What is the \"\n",
        "generated_text = generate_text(seed_text, next_words=10)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUCELf7uCVEZ",
        "outputId": "8a9bf84b-fea8-4f9f-988f-4daeb7d53e71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 475ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "What is the  program is designed to accommodate from the date of payment\n"
          ]
        }
      ]
    }
  ]
}